{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbfrL3SMDMhQ",
        "outputId": "94e16835-f884-4b9c-b5df-2138aed464e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Found existing installation: transformers 4.52.0.dev0\n",
            "Uninstalling transformers-4.52.0.dev0:\n",
            "  Successfully uninstalled transformers-4.52.0.dev0\n",
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 279325, done.\u001b[K\n",
            "remote: Counting objects: 100% (625/625), done.\u001b[K\n",
            "remote: Compressing objects: 100% (415/415), done.\u001b[K\n",
            "remote: Total 279325 (delta 407), reused 215 (delta 205), pack-reused 278700 (from 4)\u001b[K\n",
            "Receiving objects: 100% (279325/279325), 291.56 MiB | 16.65 MiB/s, done.\n",
            "Resolving deltas: 100% (207306/207306), done.\n",
            "/content/transformers\n",
            "Obtaining file:///content/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.52.0.dev0) (2024.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.52.0.dev0) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.0.dev0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.0.dev0) (2025.1.31)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building editable for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.52.0.dev0-0.editable-py3-none-any.whl size=15244 sha256=98232eb0bcb36a4c7ca26f3fd97e0746d226a5d712bd127088fd1e6874618e3e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-zb69dizl/wheels/9f/62/72/77fdff469e8308ad837268261590df9cabff9926cc4ab177c0\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "Successfully installed transformers-4.52.0.dev0\n",
            "/content/transformers/examples/pytorch/language-modeling\n",
            "Requirement already satisfied: accelerate>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (1.5.2)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (2.6.0+cu124)\n",
            "Requirement already satisfied: datasets>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (3.5.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (5.29.4)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (0.4.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (1.6.1)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (0.30.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->-r requirements.txt (line 2)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->-r requirements.txt (line 2)) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->-r requirements.txt (line 2)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->-r requirements.txt (line 2)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->-r requirements.txt (line 2)) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->-r requirements.txt (line 2)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->-r requirements.txt (line 2)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->-r requirements.txt (line 2)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->-r requirements.txt (line 2)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->-r requirements.txt (line 2)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->-r requirements.txt (line 2)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->-r requirements.txt (line 2)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->-r requirements.txt (line 2)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->-r requirements.txt (line 2)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->-r requirements.txt (line 2)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->-r requirements.txt (line 2)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->-r requirements.txt (line 2)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->-r requirements.txt (line 2)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->-r requirements.txt (line 2)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->-r requirements.txt (line 2)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.3->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.0->-r requirements.txt (line 3)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.0->-r requirements.txt (line 3)) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.0->-r requirements.txt (line 3)) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.0->-r requirements.txt (line 3)) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.0->-r requirements.txt (line 3)) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.0->-r requirements.txt (line 3)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.0->-r requirements.txt (line 3)) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.0->-r requirements.txt (line 3)) (3.11.15)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.0->-r requirements.txt (line 3)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.0->-r requirements.txt (line 3)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.0->-r requirements.txt (line 3)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.0->-r requirements.txt (line 3)) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.0->-r requirements.txt (line 3)) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.0->-r requirements.txt (line 3)) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.0->-r requirements.txt (line 3)) (1.20.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.14.0->-r requirements.txt (line 3)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.14.0->-r requirements.txt (line 3)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.14.0->-r requirements.txt (line 3)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.14.0->-r requirements.txt (line 3)) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.3->-r requirements.txt (line 2)) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.14.0->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.14.0->-r requirements.txt (line 3)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.14.0->-r requirements.txt (line 3)) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.14.0->-r requirements.txt (line 3)) (1.17.0)\n",
            "Thu Apr 24 19:54:53 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "2025-04-24 19:55:05.975249: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745524505.995238   16937 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745524506.001275   16937 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-24 19:55:06.021416: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "04/24/2025 19:55:10 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: True\n",
            "04/24/2025 19:55:10 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=None,\n",
            "eval_strategy=IntervalStrategy.NO,\n",
            "eval_use_gather_object=False,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/gpt2-wikitext-clm/runs/Apr24_19-55-10_b591dbb185b7,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=2.0,\n",
            "optim=OptimizerNames.ADAMW_TORCH,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=/content/gpt2-wikitext-clm,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=2,\n",
            "per_device_train_batch_size=2,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/gpt2-wikitext-clm,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=SaveStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "04/24/2025 19:55:20 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3\n",
            "04/24/2025 19:55:20 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3\n",
            "Found cached dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)\n",
            "04/24/2025 19:55:20 - INFO - datasets.builder - Found cached dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3\n",
            "04/24/2025 19:55:20 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3\n",
            "[INFO|configuration_utils.py:694] 2025-04-24 19:55:20,750 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
            "[INFO|configuration_utils.py:766] 2025-04-24 19:55:20,751 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.52.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:694] 2025-04-24 19:55:20,979 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
            "[INFO|configuration_utils.py:766] 2025-04-24 19:55:20,980 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.52.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2096] 2025-04-24 19:55:21,475 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2096] 2025-04-24 19:55:21,475 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2096] 2025-04-24 19:55:21,475 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2096] 2025-04-24 19:55:21,475 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2096] 2025-04-24 19:55:21,475 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2096] 2025-04-24 19:55:21,475 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2096] 2025-04-24 19:55:21,475 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|configuration_utils.py:694] 2025-04-24 19:55:21,475 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
            "[INFO|configuration_utils.py:766] 2025-04-24 19:55:21,476 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.52.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1103] 2025-04-24 19:55:21,635 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/model.safetensors\n",
            "[INFO|configuration_utils.py:1148] 2025-04-24 19:55:21,637 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:5038] 2025-04-24 19:55:21,727 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:5046] 2025-04-24 19:55:21,727 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "[INFO|configuration_utils.py:1103] 2025-04-24 19:55:21,959 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/generation_config.json\n",
            "[INFO|configuration_utils.py:1148] 2025-04-24 19:55:21,960 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256\n",
            "}\n",
            "\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-78d29071eb37c8f5.arrow\n",
            "04/24/2025 19:55:21 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-78d29071eb37c8f5.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-80498e5e02975f37.arrow\n",
            "04/24/2025 19:55:22 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-80498e5e02975f37.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-02cdcac9f6825f10.arrow\n",
            "04/24/2025 19:55:22 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-02cdcac9f6825f10.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-33a982bf8450ca1d.arrow\n",
            "04/24/2025 19:55:22 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-33a982bf8450ca1d.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-13916ac7a1a97e8e.arrow\n",
            "04/24/2025 19:55:22 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-13916ac7a1a97e8e.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-8c82566ac2db93ce.arrow\n",
            "04/24/2025 19:55:22 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-8c82566ac2db93ce.arrow\n",
            "[INFO|trainer.py:754] 2025-04-24 19:55:25,802 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2407] 2025-04-24 19:55:26,216 >> ***** Running training *****\n",
            "[INFO|trainer.py:2408] 2025-04-24 19:55:26,216 >>   Num examples = 2,318\n",
            "[INFO|trainer.py:2409] 2025-04-24 19:55:26,216 >>   Num Epochs = 2\n",
            "[INFO|trainer.py:2410] 2025-04-24 19:55:26,216 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:2413] 2025-04-24 19:55:26,216 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "[INFO|trainer.py:2414] 2025-04-24 19:55:26,216 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2415] 2025-04-24 19:55:26,216 >>   Total optimization steps = 2,318\n",
            "[INFO|trainer.py:2416] 2025-04-24 19:55:26,216 >>   Number of trainable parameters = 124,439,808\n",
            "[INFO|integration_utils.py:831] 2025-04-24 19:55:26,220 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m202011035\u001b[0m (\u001b[33mteam-achievers\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/transformers/examples/pytorch/language-modeling/wandb/run-20250424_195751-hw929yxk\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/content/gpt2-wikitext-clm\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/team-achievers/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/team-achievers/huggingface/runs/hw929yxk\u001b[0m\n",
            "  0% 0/2318 [00:00<?, ?it/s][WARNING|logging.py:328] 2025-04-24 19:57:53,718 >> `loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
            "{'loss': 3.2783, 'grad_norm': 2.610887289047241, 'learning_rate': 3.9279551337359794e-05, 'epoch': 0.43}\n",
            " 22% 500/2318 [02:09<07:45,  3.90it/s][INFO|trainer.py:3978] 2025-04-24 20:00:02,621 >> Saving model checkpoint to /content/gpt2-wikitext-clm/checkpoint-500\n",
            "[INFO|configuration_utils.py:420] 2025-04-24 20:00:02,623 >> Configuration saved in /content/gpt2-wikitext-clm/checkpoint-500/config.json\n",
            "[INFO|configuration_utils.py:917] 2025-04-24 20:00:02,624 >> Configuration saved in /content/gpt2-wikitext-clm/checkpoint-500/generation_config.json\n",
            "[INFO|modeling_utils.py:3632] 2025-04-24 20:00:05,039 >> Model weights saved in /content/gpt2-wikitext-clm/checkpoint-500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2598] 2025-04-24 20:00:05,040 >> tokenizer config file saved in /content/gpt2-wikitext-clm/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2607] 2025-04-24 20:00:05,041 >> Special tokens file saved in /content/gpt2-wikitext-clm/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 3.1731, 'grad_norm': 2.3892719745635986, 'learning_rate': 2.8494391716997416e-05, 'epoch': 0.86}\n",
            " 43% 1000/2318 [04:26<05:40,  3.87it/s][INFO|trainer.py:3978] 2025-04-24 20:02:18,932 >> Saving model checkpoint to /content/gpt2-wikitext-clm/checkpoint-1000\n",
            "[INFO|configuration_utils.py:420] 2025-04-24 20:02:18,934 >> Configuration saved in /content/gpt2-wikitext-clm/checkpoint-1000/config.json\n",
            "[INFO|configuration_utils.py:917] 2025-04-24 20:02:18,934 >> Configuration saved in /content/gpt2-wikitext-clm/checkpoint-1000/generation_config.json\n",
            "[INFO|modeling_utils.py:3632] 2025-04-24 20:02:20,688 >> Model weights saved in /content/gpt2-wikitext-clm/checkpoint-1000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2598] 2025-04-24 20:02:20,689 >> tokenizer config file saved in /content/gpt2-wikitext-clm/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2607] 2025-04-24 20:02:20,689 >> Special tokens file saved in /content/gpt2-wikitext-clm/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 3.0757, 'grad_norm': 2.292815685272217, 'learning_rate': 1.7709232096635032e-05, 'epoch': 1.29}\n",
            " 65% 1500/2318 [06:42<03:31,  3.87it/s][INFO|trainer.py:3978] 2025-04-24 20:04:35,278 >> Saving model checkpoint to /content/gpt2-wikitext-clm/checkpoint-1500\n",
            "[INFO|configuration_utils.py:420] 2025-04-24 20:04:35,280 >> Configuration saved in /content/gpt2-wikitext-clm/checkpoint-1500/config.json\n",
            "[INFO|configuration_utils.py:917] 2025-04-24 20:04:35,280 >> Configuration saved in /content/gpt2-wikitext-clm/checkpoint-1500/generation_config.json\n",
            "[INFO|modeling_utils.py:3632] 2025-04-24 20:04:38,816 >> Model weights saved in /content/gpt2-wikitext-clm/checkpoint-1500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2598] 2025-04-24 20:04:38,817 >> tokenizer config file saved in /content/gpt2-wikitext-clm/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2607] 2025-04-24 20:04:38,818 >> Special tokens file saved in /content/gpt2-wikitext-clm/checkpoint-1500/special_tokens_map.json\n",
            "{'loss': 3.0111, 'grad_norm': 2.269540309906006, 'learning_rate': 6.924072476272649e-06, 'epoch': 1.73}\n",
            " 86% 2000/2318 [08:59<01:22,  3.87it/s][INFO|trainer.py:3978] 2025-04-24 20:06:52,402 >> Saving model checkpoint to /content/gpt2-wikitext-clm/checkpoint-2000\n",
            "[INFO|configuration_utils.py:420] 2025-04-24 20:06:52,404 >> Configuration saved in /content/gpt2-wikitext-clm/checkpoint-2000/config.json\n",
            "[INFO|configuration_utils.py:917] 2025-04-24 20:06:52,404 >> Configuration saved in /content/gpt2-wikitext-clm/checkpoint-2000/generation_config.json\n",
            "[INFO|modeling_utils.py:3632] 2025-04-24 20:06:56,955 >> Model weights saved in /content/gpt2-wikitext-clm/checkpoint-2000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2598] 2025-04-24 20:06:56,956 >> tokenizer config file saved in /content/gpt2-wikitext-clm/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2607] 2025-04-24 20:06:56,957 >> Special tokens file saved in /content/gpt2-wikitext-clm/checkpoint-2000/special_tokens_map.json\n",
            "100% 2318/2318 [10:31<00:00,  3.84it/s][INFO|trainer.py:3978] 2025-04-24 20:08:23,887 >> Saving model checkpoint to /content/gpt2-wikitext-clm/checkpoint-2318\n",
            "[INFO|configuration_utils.py:420] 2025-04-24 20:08:23,889 >> Configuration saved in /content/gpt2-wikitext-clm/checkpoint-2318/config.json\n",
            "[INFO|configuration_utils.py:917] 2025-04-24 20:08:23,889 >> Configuration saved in /content/gpt2-wikitext-clm/checkpoint-2318/generation_config.json\n",
            "[INFO|modeling_utils.py:3632] 2025-04-24 20:08:25,903 >> Model weights saved in /content/gpt2-wikitext-clm/checkpoint-2318/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2598] 2025-04-24 20:08:25,904 >> tokenizer config file saved in /content/gpt2-wikitext-clm/checkpoint-2318/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2607] 2025-04-24 20:08:25,904 >> Special tokens file saved in /content/gpt2-wikitext-clm/checkpoint-2318/special_tokens_map.json\n",
            "[INFO|trainer.py:2674] 2025-04-24 20:08:30,020 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 783.8038, 'train_samples_per_second': 5.915, 'train_steps_per_second': 2.957, 'train_loss': 3.120314906649388, 'epoch': 2.0}\n",
            "100% 2318/2318 [10:37<00:00,  3.64it/s]\n",
            "[INFO|trainer.py:3978] 2025-04-24 20:08:30,025 >> Saving model checkpoint to /content/gpt2-wikitext-clm\n",
            "[INFO|configuration_utils.py:420] 2025-04-24 20:08:30,026 >> Configuration saved in /content/gpt2-wikitext-clm/config.json\n",
            "[INFO|configuration_utils.py:917] 2025-04-24 20:08:30,027 >> Configuration saved in /content/gpt2-wikitext-clm/generation_config.json\n",
            "[INFO|modeling_utils.py:3632] 2025-04-24 20:08:38,462 >> Model weights saved in /content/gpt2-wikitext-clm/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2598] 2025-04-24 20:08:38,464 >> tokenizer config file saved in /content/gpt2-wikitext-clm/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2607] 2025-04-24 20:08:38,464 >> Special tokens file saved in /content/gpt2-wikitext-clm/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        2.0\n",
            "  total_flos               =  2256314GF\n",
            "  train_loss               =     3.1203\n",
            "  train_runtime            = 0:13:03.80\n",
            "  train_samples            =       2318\n",
            "  train_samples_per_second =      5.915\n",
            "  train_steps_per_second   =      2.957\n",
            "04/24/2025 20:08:38 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:4311] 2025-04-24 20:08:38,517 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4313] 2025-04-24 20:08:38,517 >>   Num examples = 240\n",
            "[INFO|trainer.py:4316] 2025-04-24 20:08:38,517 >>   Batch size = 2\n",
            "100% 120/120 [00:11<00:00, 10.65it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        2.0\n",
            "  eval_accuracy           =     0.4276\n",
            "  eval_loss               =     3.0431\n",
            "  eval_runtime            = 0:00:11.42\n",
            "  eval_samples            =        240\n",
            "  eval_samples_per_second =     21.006\n",
            "  eval_steps_per_second   =     10.503\n",
            "  perplexity              =    20.9695\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33m/content/gpt2-wikitext-clm\u001b[0m at: \u001b[34mhttps://wandb.ai/team-achievers/huggingface/runs/hw929yxk\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250424_195751-hw929yxk/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "# 1) Make sure we start in /content\n",
        "%cd /content\n",
        "\n",
        "# 2) Uninstall any existing transformers install\n",
        "!pip uninstall -y transformers\n",
        "\n",
        "# 3) Remove any leftover “transformers” directory\n",
        "!rm -rf transformers\n",
        "\n",
        "# 4) Clone the official Transformers repo\n",
        "!git clone https://github.com/huggingface/transformers.git\n",
        "\n",
        "# 5) Enter the repo and install in editable mode\n",
        "%cd transformers\n",
        "!pip install -e .\n",
        "\n",
        "# 6) Install all other dependencies\n",
        "!pip install -q accelerate datasets evaluate sentencepiece!=0.1.92 scikit-learn\n",
        "\n",
        "# 7) Go to the GPT-2 CLM example folder and install example‐specific requirements\n",
        "%cd examples/pytorch/language-modeling\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# 8) Verify GPU is available\n",
        "!nvidia-smi\n",
        "\n",
        "# 9) Launch GPT-2 fine-tuning on WikiText-2 using all available GPUs and mixed precision\n",
        "!accelerate launch --mixed_precision fp16 run_clm.py \\\n",
        "  --model_name_or_path gpt2 \\\n",
        "  --dataset_name wikitext \\\n",
        "  --dataset_config_name wikitext-2-raw-v1 \\\n",
        "  --per_device_train_batch_size 2 \\\n",
        "  --per_device_eval_batch_size 2 \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --fp16 \\\n",
        "  --output_dir /content/gpt2-wikitext-clm \\\n",
        "  --overwrite_output_dir \\\n",
        "  --num_train_epochs 2\n",
        "# ───────────────────────────────────────────────────────────────────────────────"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezk6cobxLrKC",
        "outputId": "d5250950-f5b0-459d-d1b7-570827834fcb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "save_path = \"/content/drive/MyDrive/fine_tuned_models/gpt2-wikitext\"\n",
        "os.makedirs(save_path, exist_ok=True)"
      ],
      "metadata": {
        "id": "YijVikWIMVmc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import os\n",
        "\n",
        "# 1) The real training output\n",
        "trained_dir = \"/content/gpt2-wikitext-clm\"\n",
        "assert os.path.isdir(trained_dir), f\"{trained_dir} not found!\"\n",
        "\n",
        "# 2) Where you want to save on Drive\n",
        "save_dir = \"/content/drive/MyDrive/fine_tuned_models/my_finetuned_gpt2\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# 3) Load locally (skip HF hub)\n",
        "model = AutoModelForCausalLM.from_pretrained(trained_dir, local_files_only=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(trained_dir, local_files_only=True)\n",
        "\n",
        "# 4) Save your final model & tokenizer\n",
        "model.save_pretrained(save_dir)\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "\n",
        "print(\"✅ Saved to\", save_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWrpz4dlbv-q",
        "outputId": "1d62f244-ffa8-41b6-86a3-ecfeb49e1ca6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved to /content/drive/MyDrive/fine_tuned_models/my_finetuned_gpt2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run in a Colab cell\n",
        "!cp -r /content/gpt2-wikitext-clm /content/drive/MyDrive/fine_tuned_models/"
      ],
      "metadata": {
        "id": "hmN_M9w7MZ8E"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# 1) Your fine-tuned model folder on Drive\n",
        "model_path = \"/content/drive/MyDrive/fine_tuned_models/my_finetuned_gpt2\"\n",
        "\n",
        "# 2) Load tokenizer + model locally\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
        "model     = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
        "\n",
        "# 3) Move model to GPU if you have one\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "# 4) Tokenize input and also move tensors to the same device\n",
        "inputs = tokenizer(\"Hello AI,\", return_tensors=\"pt\").to(device)\n",
        "\n",
        "# 5) Generate\n",
        "outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "\n",
        "# 6) Decode & print\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Uk6cwM3MftQ",
        "outputId": "c43f3b30-bef5-482f-80ce-680ef05d0085"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello AI, the game 's protagonist , is a young man who has been abducted by a group of aliens . He is rescued by a group of aliens who have been sent to Earth to rescue him . \n",
            " = = Plot = = \n",
            " The game '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# after model is loaded\n",
        "num_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"{num_params/1e6:.2f}M parameters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjgmZQXFd2MN",
        "outputId": "cfa46bd2-4b59-4c21-8d33-7216efb122b9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "124.44M parameters\n"
          ]
        }
      ]
    }
  ]
}